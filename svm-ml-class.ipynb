{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Support Vector Machines</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,labels = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  1.0\n",
      "Training:  0.973214285714\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel=\"rbf\",probability=True)\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"Testing: \", clf.score(features_test, labels_test))\n",
    "print (\"Training: \", clf.score(features_train, labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with a nn, if we increase the number of layers or the number of units no considerable amount of change is observed.\n",
    "model = Sequential()\n",
    "model.add(Dense(25, activation=\"relu\", input_dim=features_train.shape[1]))\n",
    "model.add(Dense(72, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=keras.losses.categorical_crossentropy, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = to_categorical(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.5673 - acc: 0.7232\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 88us/step - loss: 0.5492 - acc: 0.7500\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 93us/step - loss: 0.5309 - acc: 0.8304\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.5111 - acc: 0.8304\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.4933 - acc: 0.7946\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.4786 - acc: 0.7946\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 127us/step - loss: 0.4668 - acc: 0.7679\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.4506 - acc: 0.8304\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 82us/step - loss: 0.4385 - acc: 0.8482\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.4280 - acc: 0.8929\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.4175 - acc: 0.8929\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 90us/step - loss: 0.4056 - acc: 0.8929\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 87us/step - loss: 0.3939 - acc: 0.8750\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.3869 - acc: 0.8482\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.3789 - acc: 0.8661\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 135us/step - loss: 0.3692 - acc: 0.9018\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 141us/step - loss: 0.3656 - acc: 0.9464\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.3579 - acc: 0.9464\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.3505 - acc: 0.9018\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 83us/step - loss: 0.3408 - acc: 0.8839\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 129us/step - loss: 0.3316 - acc: 0.9018\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 137us/step - loss: 0.3248 - acc: 0.9464\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.3178 - acc: 0.9554\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 133us/step - loss: 0.3101 - acc: 0.9375\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.3042 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 118us/step - loss: 0.2978 - acc: 0.9554\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.2948 - acc: 0.9554\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 126us/step - loss: 0.2849 - acc: 0.9554\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.2817 - acc: 0.9107\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 88us/step - loss: 0.2774 - acc: 0.9196\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 120us/step - loss: 0.2703 - acc: 0.9554\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 83us/step - loss: 0.2687 - acc: 0.9732\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 86us/step - loss: 0.2572 - acc: 0.9554\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.2566 - acc: 0.9286\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 79us/step - loss: 0.2536 - acc: 0.9375\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 90us/step - loss: 0.2430 - acc: 0.9554\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.2384 - acc: 0.9554\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 81us/step - loss: 0.2334 - acc: 0.9554\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 87us/step - loss: 0.2292 - acc: 0.9464\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.2236 - acc: 0.9554\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.2217 - acc: 0.9643\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 111us/step - loss: 0.2150 - acc: 0.9643\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 85us/step - loss: 0.2132 - acc: 0.9464\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 133us/step - loss: 0.2072 - acc: 0.9554\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 87us/step - loss: 0.2037 - acc: 0.9643\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.2001 - acc: 0.9643\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 93us/step - loss: 0.1964 - acc: 0.9643\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 115us/step - loss: 0.1929 - acc: 0.9464\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.1876 - acc: 0.9643\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.1857 - acc: 0.9732\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.1820 - acc: 0.9732\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.1783 - acc: 0.9643\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.1764 - acc: 0.9643\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 95us/step - loss: 0.1700 - acc: 0.9643\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 94us/step - loss: 0.1701 - acc: 0.9821\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 124us/step - loss: 0.1694 - acc: 0.9643\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 102us/step - loss: 0.1637 - acc: 0.9643\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 91us/step - loss: 0.1613 - acc: 0.9643\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 140us/step - loss: 0.1590 - acc: 0.9643\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.1614 - acc: 0.9643\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.1542 - acc: 0.9821\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 142us/step - loss: 0.1544 - acc: 0.9643\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 107us/step - loss: 0.1493 - acc: 0.9643\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 149us/step - loss: 0.1465 - acc: 0.9821\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 119us/step - loss: 0.1457 - acc: 0.9821\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 130us/step - loss: 0.1432 - acc: 0.9732\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.1408 - acc: 0.9732\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 86us/step - loss: 0.1385 - acc: 0.9643\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 95us/step - loss: 0.1374 - acc: 0.9643\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.1367 - acc: 0.9732\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 117us/step - loss: 0.1349 - acc: 0.9732\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.1323 - acc: 0.9643\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 109us/step - loss: 0.1300 - acc: 0.9732\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 97us/step - loss: 0.1313 - acc: 0.9643\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.1282 - acc: 0.9643\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 88us/step - loss: 0.1279 - acc: 0.9643\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.1250 - acc: 0.9643\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 106us/step - loss: 0.1244 - acc: 0.9732\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 94us/step - loss: 0.1236 - acc: 0.9732\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 99us/step - loss: 0.1205 - acc: 0.9732\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 170us/step - loss: 0.1193 - acc: 0.9732\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 110us/step - loss: 0.1178 - acc: 0.9732\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 98us/step - loss: 0.1198 - acc: 0.9643\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 113us/step - loss: 0.1158 - acc: 0.9732\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 114us/step - loss: 0.1144 - acc: 0.9732\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 101us/step - loss: 0.1134 - acc: 0.9732\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 85us/step - loss: 0.1137 - acc: 0.9732\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 89us/step - loss: 0.1121 - acc: 0.9732\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 121us/step - loss: 0.1106 - acc: 0.9732\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.1101 - acc: 0.9732\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 122us/step - loss: 0.1079 - acc: 0.9732\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 112us/step - loss: 0.1079 - acc: 0.9732\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 128us/step - loss: 0.1067 - acc: 0.9732\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 116us/step - loss: 0.1058 - acc: 0.9732\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 91us/step - loss: 0.1059 - acc: 0.9732\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 96us/step - loss: 0.1051 - acc: 0.9732\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 131us/step - loss: 0.1061 - acc: 0.9732\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 103us/step - loss: 0.1041 - acc: 0.9732\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 90us/step - loss: 0.1071 - acc: 0.9732\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 125us/step - loss: 0.1014 - acc: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120926ba8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_train, labels_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 1, 1,\n",
       "       2, 0, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = np.argmax(labels_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 983us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05862190956740003, 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = to_categorical(labels_test)\n",
    "model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.where(pred!=np.argmax(labels_test, axis=1)) # No misclassified point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_circles(n_samples=50000,noise=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[1])\n",
    "len(data[0])\n",
    "\n",
    "features, labels = data[0], data[1]\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, random_state=34, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentation\n",
    "k = Sequential()\n",
    "k.add(Dense(100, activation=\"relu\", input_dim=features.shape[1]))\n",
    "k.add(Dense(100, activation=\"relu\"))\n",
    "k.add(Dense(100, activation=\"relu\"))\n",
    "k.add(Dense(1, activation=\"sigmoid\"))\n",
    "k.compile(optimizer=\"adam\", loss=keras.losses.binary_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,601\n",
      "Trainable params: 20,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37500/37500 [==============================] - 3s 79us/step - loss: 0.6921 - acc: 0.5193\n",
      "Epoch 2/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6915 - acc: 0.5256\n",
      "Epoch 3/100\n",
      "37500/37500 [==============================] - 2s 62us/step - loss: 0.6912 - acc: 0.5271\n",
      "Epoch 4/100\n",
      "37500/37500 [==============================] - 2s 62us/step - loss: 0.6912 - acc: 0.5275\n",
      "Epoch 5/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6910 - acc: 0.5297\n",
      "Epoch 6/100\n",
      "37500/37500 [==============================] - 2s 62us/step - loss: 0.6910 - acc: 0.5281\n",
      "Epoch 7/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6910 - acc: 0.5274\n",
      "Epoch 8/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6909 - acc: 0.5290: 0s - loss: 0.6909 -\n",
      "Epoch 9/100\n",
      "37500/37500 [==============================] - 2s 66us/step - loss: 0.6908 - acc: 0.5296\n",
      "Epoch 10/100\n",
      "37500/37500 [==============================] - 3s 70us/step - loss: 0.6909 - acc: 0.5277\n",
      "Epoch 11/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6909 - acc: 0.5287\n",
      "Epoch 12/100\n",
      "37500/37500 [==============================] - 2s 64us/step - loss: 0.6907 - acc: 0.5315\n",
      "Epoch 13/100\n",
      "37500/37500 [==============================] - 2s 64us/step - loss: 0.6907 - acc: 0.5283\n",
      "Epoch 14/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6907 - acc: 0.5287\n",
      "Epoch 15/100\n",
      "37500/37500 [==============================] - 3s 72us/step - loss: 0.6907 - acc: 0.5311\n",
      "Epoch 16/100\n",
      "37500/37500 [==============================] - 2s 64us/step - loss: 0.6907 - acc: 0.5305\n",
      "Epoch 17/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6905 - acc: 0.5289\n",
      "Epoch 18/100\n",
      "37500/37500 [==============================] - 2s 62us/step - loss: 0.6906 - acc: 0.5310\n",
      "Epoch 19/100\n",
      "37500/37500 [==============================] - 2s 63us/step - loss: 0.6906 - acc: 0.5300\n",
      "Epoch 20/100\n",
      "37500/37500 [==============================] - 2s 64us/step - loss: 0.6906 - acc: 0.5309\n",
      "Epoch 21/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6907 - acc: 0.5279\n",
      "Epoch 22/100\n",
      "37500/37500 [==============================] - 3s 67us/step - loss: 0.6906 - acc: 0.5294\n",
      "Epoch 23/100\n",
      "37500/37500 [==============================] - 2s 64us/step - loss: 0.6906 - acc: 0.5304\n",
      "Epoch 24/100\n",
      "37500/37500 [==============================] - 3s 69us/step - loss: 0.6906 - acc: 0.5300\n",
      "Epoch 25/100\n",
      "37500/37500 [==============================] - 2s 62us/step - loss: 0.6906 - acc: 0.5307\n",
      "Epoch 26/100\n",
      "37500/37500 [==============================] - 3s 69us/step - loss: 0.6906 - acc: 0.5304\n",
      "Epoch 27/100\n",
      "37500/37500 [==============================] - 3s 70us/step - loss: 0.6903 - acc: 0.5318\n",
      "Epoch 28/100\n",
      "37500/37500 [==============================] - 3s 72us/step - loss: 0.6905 - acc: 0.5322\n",
      "Epoch 29/100\n",
      "37500/37500 [==============================] - 3s 70us/step - loss: 0.6906 - acc: 0.5297\n",
      "Epoch 30/100\n",
      "37500/37500 [==============================] - 3s 74us/step - loss: 0.6904 - acc: 0.5325\n",
      "Epoch 31/100\n",
      "37500/37500 [==============================] - 3s 73us/step - loss: 0.6904 - acc: 0.5321\n",
      "Epoch 32/100\n",
      "37500/37500 [==============================] - 3s 68us/step - loss: 0.6904 - acc: 0.5324\n",
      "Epoch 33/100\n",
      "37500/37500 [==============================] - 3s 74us/step - loss: 0.6904 - acc: 0.5315\n",
      "Epoch 34/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6904 - acc: 0.5315\n",
      "Epoch 35/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6904 - acc: 0.5325\n",
      "Epoch 36/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6903 - acc: 0.5308\n",
      "Epoch 37/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6904 - acc: 0.5314\n",
      "Epoch 38/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6903 - acc: 0.5321\n",
      "Epoch 39/100\n",
      "37500/37500 [==============================] - 3s 77us/step - loss: 0.6903 - acc: 0.5322: 0s - loss: 0.6903 - acc: 0.53\n",
      "Epoch 40/100\n",
      "37500/37500 [==============================] - 3s 76us/step - loss: 0.6903 - acc: 0.5306\n",
      "Epoch 41/100\n",
      "37500/37500 [==============================] - 3s 70us/step - loss: 0.6903 - acc: 0.5308\n",
      "Epoch 42/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6902 - acc: 0.5334\n",
      "Epoch 43/100\n",
      "37500/37500 [==============================] - 3s 73us/step - loss: 0.6903 - acc: 0.5329\n",
      "Epoch 44/100\n",
      "37500/37500 [==============================] - 2s 65us/step - loss: 0.6902 - acc: 0.5332\n",
      "Epoch 45/100\n",
      "37500/37500 [==============================] - 2s 66us/step - loss: 0.6903 - acc: 0.5314\n",
      "Epoch 46/100\n",
      "28640/37500 [=====================>........] - ETA: 0s - loss: 0.6901 - acc: 0.5339"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-bfe9598ea3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m           \u001b[0mfeed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m\"\"\"The string name of this tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1786\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k.fit(features_train, labels_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34972792, -1.57921755],\n",
       "       [-0.23643763,  0.38566625],\n",
       "       [-0.65857801, -0.99498635],\n",
       "       ..., \n",
       "       [ 0.29659331, -1.64319431],\n",
       "       [-0.98824319,  1.20133493],\n",
       "       [-1.02891853,  1.63337717]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47361779, -0.2658943 ],\n",
       "       [-0.43042046,  0.12051038],\n",
       "       [-1.0933412 , -1.57985635],\n",
       "       ..., \n",
       "       [ 1.15149605, -1.50401462],\n",
       "       [ 1.82553073, -1.01452914],\n",
       "       [ 3.10851979, -1.44037792]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34972792, -1.57921755],\n",
       "       [-0.23643763,  0.38566625],\n",
       "       [-0.65857801, -0.99498635],\n",
       "       ..., \n",
       "       [ 0.29659331, -1.64319431],\n",
       "       [-0.98824319,  1.20133493],\n",
       "       [-1.02891853,  1.63337717]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C is a regularization.\n",
    "# We need to choose a middle value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM objective is to maximize the margin.\n",
    "# The margin is defined as the distance between the separating decision boundary and the training samples that are closest to the hyperplance which are called support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.40106928, -0.20836932,  0.80730004],\n",
       "       [ 2.3880326 , -0.19771186,  0.80967927],\n",
       "       [ 2.39666854, -0.21830903,  0.82164049],\n",
       "       [ 2.37644346, -0.19487104,  0.81842758],\n",
       "       [ 2.40076876, -0.21306488,  0.81229612],\n",
       "       [ 2.3642535 ,  0.84146058, -0.20571408],\n",
       "       [ 2.38542461, -0.20988417,  0.82445957],\n",
       "       [ 2.39400596,  0.80269871, -0.19670468],\n",
       "       [ 2.36585265, -0.19949048,  0.83363783],\n",
       "       [ 2.38424031,  0.80707165, -0.19131196],\n",
       "       [ 2.38591954,  0.81047863, -0.19639817],\n",
       "       [ 2.38071671,  0.81130315, -0.19201986],\n",
       "       [ 2.3874413 , -0.20367587,  0.81623457],\n",
       "       [ 2.3670745 , -0.22211227,  0.85503777],\n",
       "       [ 2.35292313, -0.19247054,  0.83954741],\n",
       "       [ 2.3396232 , -0.17270891,  0.8330857 ],\n",
       "       [ 2.38702925, -0.20372883,  0.81669958],\n",
       "       [ 2.40017696,  0.79537627, -0.19555323],\n",
       "       [ 2.34664481,  0.85684755, -0.20349236],\n",
       "       [ 2.39168928,  0.80033838, -0.19202766],\n",
       "       [ 2.35824746,  0.85254345, -0.21079091],\n",
       "       [ 2.39058546,  0.80682894, -0.1974144 ],\n",
       "       [ 2.37844331, -0.23200547,  0.85356216],\n",
       "       [ 2.35237999,  0.86262619, -0.21500618],\n",
       "       [ 2.33553653,  0.86319531, -0.19873184],\n",
       "       [ 2.36577659,  0.83504828, -0.20082488],\n",
       "       [ 2.37815726,  0.82770289, -0.20586015],\n",
       "       [ 2.39260932,  0.80687566, -0.19948498],\n",
       "       [ 2.39705809,  0.79830141, -0.1953595 ],\n",
       "       [ 2.37297711,  0.81649546, -0.18947257],\n",
       "       [ 2.37215182,  0.8228492 , -0.19500102],\n",
       "       [ 2.37450466,  0.83134792, -0.20585259],\n",
       "       [ 2.37644491, -0.20282064,  0.82637573],\n",
       "       [ 2.36808308, -0.19694427,  0.82886119],\n",
       "       [ 2.38424031,  0.80707165, -0.19131196],\n",
       "       [ 2.4032044 , -0.22098239,  0.81777799],\n",
       "       [ 2.38446456, -0.19876109,  0.81429653],\n",
       "       [ 2.38424031,  0.80707165, -0.19131196],\n",
       "       [ 2.37445393, -0.2115921 ,  0.83713816],\n",
       "       [ 2.39305769,  0.80617603, -0.19923372],\n",
       "       [ 2.40437241, -0.21699445,  0.81262203],\n",
       "       [ 2.33435658, -0.17923247,  0.84487589],\n",
       "       [ 2.37605748, -0.21568225,  0.83962476],\n",
       "       [ 2.36594927,  0.83778105, -0.20373032],\n",
       "       [ 2.34139382,  0.86799986, -0.20939369],\n",
       "       [ 2.38577953, -0.1960535 ,  0.81027396],\n",
       "       [ 2.38488052,  0.80830054, -0.19318107],\n",
       "       [ 2.38653573, -0.20925492,  0.82271918],\n",
       "       [ 2.39053208,  0.80567561, -0.1962077 ],\n",
       "       [ 2.39908438,  0.79401671, -0.19310109],\n",
       "       [-0.29258276,  2.28290408,  1.00967868],\n",
       "       [-0.3269501 ,  2.36540282,  0.96154728],\n",
       "       [-0.31603148,  2.21752388,  1.0985076 ],\n",
       "       [-0.35109953,  2.39623373,  0.9548658 ],\n",
       "       [-0.34300425,  2.31070871,  1.03229554],\n",
       "       [-0.36406296,  2.37573658,  0.98832637],\n",
       "       [-0.33347266,  2.29100827,  1.04246439],\n",
       "       [-0.21194244,  2.3395798 ,  0.87236264],\n",
       "       [-0.32448959,  2.3560958 ,  0.9683938 ],\n",
       "       [-0.33768886,  2.40341339,  0.93427548],\n",
       "       [-0.2562883 ,  2.32711463,  0.92917368],\n",
       "       [-0.34502409,  2.43088082,  0.91414326],\n",
       "       [-0.30931434,  2.43831952,  0.87099482],\n",
       "       [-0.35821911,  2.31958315,  1.03863597],\n",
       "       [-0.27816626,  2.48059985,  0.79756641],\n",
       "       [-0.3038933 ,  2.38303927,  0.92085403],\n",
       "       [-0.36072912,  2.33990829,  1.02082083],\n",
       "       [-0.32543978,  2.49584093,  0.82959885],\n",
       "       [-0.34409556,  2.26130337,  1.08279219],\n",
       "       [-0.32770152,  2.46989101,  0.85781051],\n",
       "       [-0.34888592,  2.1949687 ,  1.15391721],\n",
       "       [-0.32228453,  2.48831995,  0.83396458],\n",
       "       [-0.35206505,  1.18096831,  2.17109674],\n",
       "       [-0.349183  ,  2.35238268,  0.99680033],\n",
       "       [-0.32353378,  2.43960083,  0.88393295],\n",
       "       [-0.31645722,  2.39021783,  0.92623939],\n",
       "       [-0.32192817,  2.25716713,  1.06476104],\n",
       "       [-0.33844192,  1.14341324,  2.19502869],\n",
       "       [-0.36244132,  2.3550734 ,  1.00736792],\n",
       "       [-0.25414311,  2.48113406,  0.77300905],\n",
       "       [-0.31937123,  2.45171209,  0.86765914],\n",
       "       [-0.29725428,  2.45748939,  0.8397649 ],\n",
       "       [-0.32367228,  2.49612191,  0.82755037],\n",
       "       [-0.35807253,  1.09893489,  2.25913764],\n",
       "       [-0.35683625,  2.3194382 ,  1.03739805],\n",
       "       [-0.32375582,  2.34821619,  0.97553963],\n",
       "       [-0.32382936,  2.28986289,  1.03396647],\n",
       "       [-0.33396954,  2.3471679 ,  0.98680163],\n",
       "       [-0.33374767,  2.46590075,  0.86784692],\n",
       "       [-0.35215584,  2.42667672,  0.92547913],\n",
       "       [-0.36270274,  2.37841026,  0.98429248],\n",
       "       [-0.35254017,  2.35903099,  0.99350917],\n",
       "       [-0.33753856,  2.47760437,  0.85993419],\n",
       "       [-0.21960429,  2.34799263,  0.87161166],\n",
       "       [-0.35973408,  2.42881153,  0.93092255],\n",
       "       [-0.33372715,  2.47442886,  0.85929829],\n",
       "       [-0.3479649 ,  2.45447581,  0.89348909],\n",
       "       [-0.33607647,  2.44857825,  0.88749822],\n",
       "       [-0.15119993,  2.32351948,  0.82768045],\n",
       "       [-0.34707949,  2.4637556 ,  0.8833239 ],\n",
       "       [-0.26422887,  0.76609617,  2.49813269],\n",
       "       [-0.35394701,  1.00687278,  2.34707423],\n",
       "       [-0.30046054,  0.84343528,  2.45702526],\n",
       "       [-0.32826578,  0.91429693,  2.41396886],\n",
       "       [-0.30748884,  0.80748884,  2.5       ],\n",
       "       [-0.24680111,  0.83469124,  2.41210987],\n",
       "       [-0.34903619,  1.16033027,  2.18870592],\n",
       "       [-0.27186095,  0.84580522,  2.42605573],\n",
       "       [-0.30263188,  0.85698942,  2.44564246],\n",
       "       [-0.25639136,  0.82684248,  2.42954888],\n",
       "       [-0.33587133,  1.04762208,  2.28824925],\n",
       "       [-0.34322851,  0.96360993,  2.37961858],\n",
       "       [-0.32341426,  0.90315908,  2.42025519],\n",
       "       [-0.35041664,  0.985784  ,  2.36463265],\n",
       "       [-0.32049877,  0.89125618,  2.42924259],\n",
       "       [-0.31888511,  0.91453591,  2.4043492 ],\n",
       "       [-0.33346839,  0.95870202,  2.37476637],\n",
       "       [-0.21535858,  0.85912434,  2.35623424],\n",
       "       [-0.21178248,  0.84273818,  2.36904429],\n",
       "       [-0.34116116,  1.09498421,  2.24617694],\n",
       "       [-0.30289927,  0.84709705,  2.45580222],\n",
       "       [-0.3544807 ,  1.04973912,  2.30474158],\n",
       "       [-0.23323163,  0.8459551 ,  2.38727653],\n",
       "       [-0.35937265,  1.12254602,  2.23682664],\n",
       "       [-0.30943829,  0.87762512,  2.43181317],\n",
       "       [-0.29114204,  0.88598586,  2.40515618],\n",
       "       [-0.36352875,  1.16651089,  2.19701785],\n",
       "       [-0.35918098,  1.14993262,  2.20924836],\n",
       "       [-0.3230477 ,  0.84413176,  2.47891594],\n",
       "       [-0.2956774 ,  0.94351118,  2.35216622],\n",
       "       [-0.27824491,  0.85766435,  2.42058056],\n",
       "       [-0.22178614,  0.88217827,  2.33960787],\n",
       "       [-0.31857016,  0.82498851,  2.49358166],\n",
       "       [-0.35102022,  1.14556593,  2.20545429],\n",
       "       [-0.31248862,  0.97182035,  2.34066827],\n",
       "       [-0.26060237,  0.84912582,  2.41147655],\n",
       "       [-0.28932157,  0.84452959,  2.44479198],\n",
       "       [-0.3313998 ,  0.96486719,  2.36653261],\n",
       "       [-0.36151461,  1.18206974,  2.17944487],\n",
       "       [-0.32073175,  0.93813444,  2.38259731],\n",
       "       [-0.30508519,  0.83203901,  2.47304617],\n",
       "       [-0.31075385,  0.97105372,  2.33970013],\n",
       "       [-0.35394701,  1.00687278,  2.34707423],\n",
       "       [-0.29549042,  0.81108615,  2.48440426],\n",
       "       [-0.28848171,  0.81873013,  2.46975158],\n",
       "       [-0.32140492,  0.93316628,  2.38823864],\n",
       "       [-0.35016329,  1.03716402,  2.31299928],\n",
       "       [-0.34178323,  1.00000361,  2.34177961],\n",
       "       [-0.30198656,  0.90402128,  2.39796528],\n",
       "       [-0.35218266,  1.07297397,  2.27920869]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(features) # Used to find distance of the data point from decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.1,  3.5,  1.4,  0.2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(clf.predict_proba(features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
