{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x10ab2bd30>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y + 12 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 122.,  122.],\n",
       "        [ 122.,  122.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x10a8daac8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 122.,  122.],\n",
       "        [ 122.,  122.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 122.,  122.],\n",
       "        [ 122.,  122.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ab75bb780f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(122.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(122.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x*x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.],\n",
       "        [ 3.,  3.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x10aad50b8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.],\n",
       "        [ 3.,  3.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x10aad50b8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000,  0.5000],\n",
       "        [ 0.5000,  0.5000]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "input_dim = 1000\n",
    "H = 100\n",
    "output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, input_dim)\n",
    "y = torch.randn(batch_size, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1000])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential( \n",
    "    torch.nn.Linear(input_dim, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, output)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.5510780811309814\n",
      "1 2.497006893157959\n",
      "2 2.4442248344421387\n",
      "3 2.3924505710601807\n",
      "4 2.3415372371673584\n",
      "5 2.2913715839385986\n",
      "6 2.241933822631836\n",
      "7 2.193284273147583\n",
      "8 2.145406723022461\n",
      "9 2.0983524322509766\n",
      "10 2.052091598510742\n",
      "11 2.006612777709961\n",
      "12 1.9619697332382202\n",
      "13 1.918089509010315\n",
      "14 1.8750063180923462\n",
      "15 1.8327534198760986\n",
      "16 1.7911783456802368\n",
      "17 1.7504388093948364\n",
      "18 1.7104668617248535\n",
      "19 1.671191692352295\n",
      "20 1.6326886415481567\n",
      "21 1.5948905944824219\n",
      "22 1.557816505432129\n",
      "23 1.5213748216629028\n",
      "24 1.4857076406478882\n",
      "25 1.4506853818893433\n",
      "26 1.4163668155670166\n",
      "27 1.3826943635940552\n",
      "28 1.3497059345245361\n",
      "29 1.3173702955245972\n",
      "30 1.2856372594833374\n",
      "31 1.2545489072799683\n",
      "32 1.2241400480270386\n",
      "33 1.1943297386169434\n",
      "34 1.165114402770996\n",
      "35 1.1364916563034058\n",
      "36 1.1084734201431274\n",
      "37 1.0810191631317139\n",
      "38 1.0541400909423828\n",
      "39 1.0278358459472656\n",
      "40 1.0020848512649536\n",
      "41 0.976894199848175\n",
      "42 0.9522259831428528\n",
      "43 0.9280776977539062\n",
      "44 0.9044792652130127\n",
      "45 0.8814493417739868\n",
      "46 0.8588540554046631\n",
      "47 0.836775004863739\n",
      "48 0.8151828050613403\n",
      "49 0.7940934896469116\n",
      "50 0.7734453082084656\n",
      "51 0.7532685399055481\n",
      "52 0.7335396409034729\n",
      "53 0.714242160320282\n",
      "54 0.6953778862953186\n",
      "55 0.6769338250160217\n",
      "56 0.6589239835739136\n",
      "57 0.6413069367408752\n",
      "58 0.6241110563278198\n",
      "59 0.607299268245697\n",
      "60 0.5908722877502441\n",
      "61 0.5748449563980103\n",
      "62 0.5591683387756348\n",
      "63 0.5438475608825684\n",
      "64 0.528888463973999\n",
      "65 0.5142977833747864\n",
      "66 0.5000389218330383\n",
      "67 0.4861442446708679\n",
      "68 0.4725968539714813\n",
      "69 0.4593786299228668\n",
      "70 0.44645607471466064\n",
      "71 0.43385443091392517\n",
      "72 0.421555757522583\n",
      "73 0.4095611274242401\n",
      "74 0.3978472650051117\n",
      "75 0.38642144203186035\n",
      "76 0.3752765953540802\n",
      "77 0.364406943321228\n",
      "78 0.35380518436431885\n",
      "79 0.34348395466804504\n",
      "80 0.33340972661972046\n",
      "81 0.32359418272972107\n",
      "82 0.3140254318714142\n",
      "83 0.30468323826789856\n",
      "84 0.29558268189430237\n",
      "85 0.28671786189079285\n",
      "86 0.2780720293521881\n",
      "87 0.2696414589881897\n",
      "88 0.2614109516143799\n",
      "89 0.2533879578113556\n",
      "90 0.24556712806224823\n",
      "91 0.23794832825660706\n",
      "92 0.23052868247032166\n",
      "93 0.2233150601387024\n",
      "94 0.21632523834705353\n",
      "95 0.20952585339546204\n",
      "96 0.20291805267333984\n",
      "97 0.19648271799087524\n",
      "98 0.19022753834724426\n",
      "99 0.18415309488773346\n",
      "100 0.1782442182302475\n",
      "101 0.17251257598400116\n",
      "102 0.16693547368049622\n",
      "103 0.16152966022491455\n",
      "104 0.15627527236938477\n",
      "105 0.15118525922298431\n",
      "106 0.14623159170150757\n",
      "107 0.14143551886081696\n",
      "108 0.13677917420864105\n",
      "109 0.13225948810577393\n",
      "110 0.1278754472732544\n",
      "111 0.12362388521432877\n",
      "112 0.11949749290943146\n",
      "113 0.11549829691648483\n",
      "114 0.11161994934082031\n",
      "115 0.10785718262195587\n",
      "116 0.10420539975166321\n",
      "117 0.10066909343004227\n",
      "118 0.09724072366952896\n",
      "119 0.09391634166240692\n",
      "120 0.0906933918595314\n",
      "121 0.08757036179304123\n",
      "122 0.08454746007919312\n",
      "123 0.08161225914955139\n",
      "124 0.07877430319786072\n",
      "125 0.07602546364068985\n",
      "126 0.07335939258337021\n",
      "127 0.07078240066766739\n",
      "128 0.06828756630420685\n",
      "129 0.06587330251932144\n",
      "130 0.06353293359279633\n",
      "131 0.061269454658031464\n",
      "132 0.05907829478383064\n",
      "133 0.056959643959999084\n",
      "134 0.05490966886281967\n",
      "135 0.052927255630493164\n",
      "136 0.051010336726903915\n",
      "137 0.04915699362754822\n",
      "138 0.04736325889825821\n",
      "139 0.04563067480921745\n",
      "140 0.04395395517349243\n",
      "141 0.042335327714681625\n",
      "142 0.04076994210481644\n",
      "143 0.03925858065485954\n",
      "144 0.03779686614871025\n",
      "145 0.03638583794236183\n",
      "146 0.03502379357814789\n",
      "147 0.03370748460292816\n",
      "148 0.032438211143016815\n",
      "149 0.03121313825249672\n",
      "150 0.03003101237118244\n",
      "151 0.028889229521155357\n",
      "152 0.027786830440163612\n",
      "153 0.02672228403389454\n",
      "154 0.025696344673633575\n",
      "155 0.02470746450126171\n",
      "156 0.02375151589512825\n",
      "157 0.02282896265387535\n",
      "158 0.021940132603049278\n",
      "159 0.02108290232717991\n",
      "160 0.02025659568607807\n",
      "161 0.019460396841168404\n",
      "162 0.018692880868911743\n",
      "163 0.01795269176363945\n",
      "164 0.017239881679415703\n",
      "165 0.01655300334095955\n",
      "166 0.0158917848020792\n",
      "167 0.015255048871040344\n",
      "168 0.014641452580690384\n",
      "169 0.014050707221031189\n",
      "170 0.013482203707098961\n",
      "171 0.012934733182191849\n",
      "172 0.012407297268509865\n",
      "173 0.011900260113179684\n",
      "174 0.011412220075726509\n",
      "175 0.010943224653601646\n",
      "176 0.010491417720913887\n",
      "177 0.010057481937110424\n",
      "178 0.009639679454267025\n",
      "179 0.009238388389348984\n",
      "180 0.008851964958012104\n",
      "181 0.00848076306283474\n",
      "182 0.008124059066176414\n",
      "183 0.0077811251394450665\n",
      "184 0.00745139317587018\n",
      "185 0.007134770974516869\n",
      "186 0.0068308147601783276\n",
      "187 0.006538058165460825\n",
      "188 0.006257254164665937\n",
      "189 0.0059878029860556126\n",
      "190 0.005728961434215307\n",
      "191 0.005480474326759577\n",
      "192 0.005242651794105768\n",
      "193 0.005013673100620508\n",
      "194 0.004794396460056305\n",
      "195 0.004584066104143858\n",
      "196 0.0043822661973536015\n",
      "197 0.004188749939203262\n",
      "198 0.004003170412033796\n",
      "199 0.003825236577540636\n",
      "200 0.003654695814475417\n",
      "201 0.00349125056527555\n",
      "202 0.003334560664370656\n",
      "203 0.003184529719874263\n",
      "204 0.003040789160877466\n",
      "205 0.0029029790312051773\n",
      "206 0.002771034836769104\n",
      "207 0.0026448324788361788\n",
      "208 0.002523834118619561\n",
      "209 0.0024081531446427107\n",
      "210 0.002297421917319298\n",
      "211 0.0021914448589086533\n",
      "212 0.002090094843879342\n",
      "213 0.00199314602650702\n",
      "214 0.0019002478802576661\n",
      "215 0.0018115744460374117\n",
      "216 0.0017267292132601142\n",
      "217 0.0016456362791359425\n",
      "218 0.0015681724762544036\n",
      "219 0.001494033494964242\n",
      "220 0.0014232172397896647\n",
      "221 0.0013555592158809304\n",
      "222 0.001290905405767262\n",
      "223 0.0012291307793930173\n",
      "224 0.0011701445328071713\n",
      "225 0.0011138145346194506\n",
      "226 0.0010600307723507285\n",
      "227 0.0010087043046951294\n",
      "228 0.0009597006719559431\n",
      "229 0.0009130329126492143\n",
      "230 0.0008683290216140449\n",
      "231 0.0008257501176558435\n",
      "232 0.0007851950940676033\n",
      "233 0.0007464413647539914\n",
      "234 0.0007095547625795007\n",
      "235 0.0006743291160091758\n",
      "236 0.0006407490000128746\n",
      "237 0.0006087825167924166\n",
      "238 0.0005782937514595687\n",
      "239 0.0005492605268955231\n",
      "240 0.0005215872079133987\n",
      "241 0.0004952375893481076\n",
      "242 0.00047014959272928536\n",
      "243 0.0004462543874979019\n",
      "244 0.00042352182208560407\n",
      "245 0.0004018544277641922\n",
      "246 0.00038124166894704103\n",
      "247 0.0003616321482695639\n",
      "248 0.0003429853531997651\n",
      "249 0.00032522756373509765\n",
      "250 0.00030834958306513727\n",
      "251 0.00029229794745333493\n",
      "252 0.0002770375576801598\n",
      "253 0.00026253462419845164\n",
      "254 0.0002487473248038441\n",
      "255 0.00023563753347843885\n",
      "256 0.00022319321578834206\n",
      "257 0.00021136205759830773\n",
      "258 0.00020012845925521106\n",
      "259 0.00018946017371490598\n",
      "260 0.00017933252092916518\n",
      "261 0.00016970635624602437\n",
      "262 0.0001605771540198475\n",
      "263 0.00015191840066108853\n",
      "264 0.00014369252312462777\n",
      "265 0.00013589239097200334\n",
      "266 0.0001284939789911732\n",
      "267 0.00012148018868174404\n",
      "268 0.00011483062553452328\n",
      "269 0.00010852320701815188\n",
      "270 0.00010254458175040781\n",
      "271 9.687915007816628e-05\n",
      "272 9.151284757535905e-05\n",
      "273 8.6426836787723e-05\n",
      "274 8.160636207321659e-05\n",
      "275 7.704641029704362e-05\n",
      "276 7.272185030160472e-05\n",
      "277 6.863246380817145e-05\n",
      "278 6.47634151391685e-05\n",
      "279 6.110435060691088e-05\n",
      "280 5.763008812209591e-05\n",
      "281 5.43529968126677e-05\n",
      "282 5.125246025272645e-05\n",
      "283 4.831676778849214e-05\n",
      "284 4.5545169996330515e-05\n",
      "285 4.2921943531837314e-05\n",
      "286 4.04428101319354e-05\n",
      "287 3.810076304944232e-05\n",
      "288 3.588755498640239e-05\n",
      "289 3.379680492798798e-05\n",
      "290 3.182256477884948e-05\n",
      "291 2.995571048813872e-05\n",
      "292 2.819583096425049e-05\n",
      "293 2.6534213247941807e-05\n",
      "294 2.4963534087873995e-05\n",
      "295 2.348412999708671e-05\n",
      "296 2.2087853722041473e-05\n",
      "297 2.0768227841472253e-05\n",
      "298 1.9526429241523147e-05\n",
      "299 1.8354703570366837e-05\n",
      "300 1.7248989024665207e-05\n",
      "301 1.6208603483391926e-05\n",
      "302 1.5226288269332144e-05\n",
      "303 1.4303453099273611e-05\n",
      "304 1.3431012121145613e-05\n",
      "305 1.261049328604713e-05\n",
      "306 1.1838547834486235e-05\n",
      "307 1.111153051169822e-05\n",
      "308 1.0425927939650137e-05\n",
      "309 9.781063454283867e-06\n",
      "310 9.17497254704358e-06\n",
      "311 8.604791219113395e-06\n",
      "312 8.066994269029237e-06\n",
      "313 7.563852250314085e-06\n",
      "314 7.08784637026838e-06\n",
      "315 6.642911102971993e-06\n",
      "316 6.223127456905786e-06\n",
      "317 5.8292866924603e-06\n",
      "318 5.457894076243974e-06\n",
      "319 5.110799065732863e-06\n",
      "320 4.784560587722808e-06\n",
      "321 4.47825505034416e-06\n",
      "322 4.19026764575392e-06\n",
      "323 3.920880772056989e-06\n",
      "324 3.667080818559043e-06\n",
      "325 3.429580146985245e-06\n",
      "326 3.2066282074083574e-06\n",
      "327 2.997526053150068e-06\n",
      "328 2.8016870601277333e-06\n",
      "329 2.6176167011726648e-06\n",
      "330 2.4453545393043896e-06\n",
      "331 2.2838073618913768e-06\n",
      "332 2.13309772334469e-06\n",
      "333 1.991170165638323e-06\n",
      "334 1.8581426957098302e-06\n",
      "335 1.7345410014968365e-06\n",
      "336 1.6183121260837652e-06\n",
      "337 1.5093849015102023e-06\n",
      "338 1.4075161516302614e-06\n",
      "339 1.3123748203724972e-06\n",
      "340 1.223448180098785e-06\n",
      "341 1.1403616326788324e-06\n",
      "342 1.0625082040860434e-06\n",
      "343 9.897455583995907e-07\n",
      "344 9.218990157933149e-07\n",
      "345 8.582160830883367e-07\n",
      "346 7.989792720763944e-07\n",
      "347 7.436441933350579e-07\n",
      "348 6.921404178683588e-07\n",
      "349 6.436749231397698e-07\n",
      "350 5.986649966871482e-07\n",
      "351 5.567521839111578e-07\n",
      "352 5.17604235028557e-07\n",
      "353 4.809027132068877e-07\n",
      "354 4.469883094770921e-07\n",
      "355 4.1532305772307154e-07\n",
      "356 3.857686863284471e-07\n",
      "357 3.5815133969663293e-07\n",
      "358 3.323516466480214e-07\n",
      "359 3.085343109887617e-07\n",
      "360 2.8622727654692426e-07\n",
      "361 2.655621074154624e-07\n",
      "362 2.4621141392344725e-07\n",
      "363 2.2833667401300772e-07\n",
      "364 2.1164967733966478e-07\n",
      "365 1.9618981639268895e-07\n",
      "366 1.8168245219385426e-07\n",
      "367 1.683268777696867e-07\n",
      "368 1.5592577540246566e-07\n",
      "369 1.4422421656945517e-07\n",
      "370 1.3357529837776383e-07\n",
      "371 1.2362359314010973e-07\n",
      "372 1.1432091895358099e-07\n",
      "373 1.0570404640475317e-07\n",
      "374 9.76889467096953e-08\n",
      "375 9.021384528296039e-08\n",
      "376 8.341167045955444e-08\n",
      "377 7.705105531385925e-08\n",
      "378 7.116634748172146e-08\n",
      "379 6.569204202833134e-08\n",
      "380 6.07073076253073e-08\n",
      "381 5.603781971785793e-08\n",
      "382 5.165513883298445e-08\n",
      "383 4.76068677812691e-08\n",
      "384 4.390669516851631e-08\n",
      "385 4.050276913858397e-08\n",
      "386 3.730269071411385e-08\n",
      "387 3.437560636143644e-08\n",
      "388 3.16943733480457e-08\n",
      "389 2.9166150383730383e-08\n",
      "390 2.686807576424144e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 2.4727482283992686e-08\n",
      "392 2.2731082793825408e-08\n",
      "393 2.0944455059179745e-08\n",
      "394 1.922098213924528e-08\n",
      "395 1.7689401943243865e-08\n",
      "396 1.6212791109637692e-08\n",
      "397 1.4903672962418568e-08\n",
      "398 1.3702515566649254e-08\n",
      "399 1.2617770828171615e-08\n",
      "400 1.1560394419518616e-08\n",
      "401 1.0624614077414662e-08\n",
      "402 9.769951958560341e-09\n",
      "403 8.980872046038257e-09\n",
      "404 8.247962313134849e-09\n",
      "405 7.583072836325755e-09\n",
      "406 6.942618036021031e-09\n",
      "407 6.3949205930668995e-09\n",
      "408 5.8843721006951455e-09\n",
      "409 5.392840840556801e-09\n",
      "410 4.9669184321032844e-09\n",
      "411 4.565849920368237e-09\n",
      "412 4.201774039813699e-09\n",
      "413 3.866412967568067e-09\n",
      "414 3.557236727402824e-09\n",
      "415 3.2876954492166988e-09\n",
      "416 3.0380931104190267e-09\n",
      "417 2.8018822817443834e-09\n",
      "418 2.592659198441538e-09\n",
      "419 2.385773134605529e-09\n",
      "420 2.1832242680375202e-09\n",
      "421 2.036546709049958e-09\n",
      "422 1.8746386665213777e-09\n",
      "423 1.7367796090184129e-09\n",
      "424 1.6172345684850598e-09\n",
      "425 1.4999566033679912e-09\n",
      "426 1.382354453127732e-09\n",
      "427 1.2984811004201902e-09\n",
      "428 1.2011904804154483e-09\n",
      "429 1.120367243423459e-09\n",
      "430 1.0439822339947114e-09\n",
      "431 9.811316203922615e-10\n",
      "432 9.16184794697017e-10\n",
      "433 8.555699482215573e-10\n",
      "434 8.070918933178461e-10\n",
      "435 7.516930966566804e-10\n",
      "436 7.096354059932253e-10\n",
      "437 6.626516557695084e-10\n",
      "438 6.328391699561564e-10\n",
      "439 5.86048765072178e-10\n",
      "440 5.571840766549485e-10\n",
      "441 5.243417366962433e-10\n",
      "442 5.000550529210557e-10\n",
      "443 4.717914947605095e-10\n",
      "444 4.532598185669201e-10\n",
      "445 4.324722246984436e-10\n",
      "446 4.1317438359556036e-10\n",
      "447 3.9164127496604806e-10\n",
      "448 3.7190889257132653e-10\n",
      "449 3.5905184381235244e-10\n",
      "450 3.4419672667596046e-10\n",
      "451 3.3134156529612824e-10\n",
      "452 3.15679371531985e-10\n",
      "453 3.070914078584508e-10\n",
      "454 2.908556728797862e-10\n",
      "455 2.792669429041439e-10\n",
      "456 2.731388448751204e-10\n",
      "457 2.6446089762544034e-10\n",
      "458 2.525316622481455e-10\n",
      "459 2.4617524685410785e-10\n",
      "460 2.40089059744264e-10\n",
      "461 2.3075735766653338e-10\n",
      "462 2.2346570427433932e-10\n",
      "463 2.1811652484160504e-10\n",
      "464 2.1441531883326093e-10\n",
      "465 2.0548540646814217e-10\n",
      "466 2.0006882262002534e-10\n",
      "467 1.9826659758415133e-10\n",
      "468 1.8938602075468935e-10\n",
      "469 1.8737891793740857e-10\n",
      "470 1.8225780606950792e-10\n",
      "471 1.7895025739012027e-10\n",
      "472 1.7258193485414353e-10\n",
      "473 1.6891096854543264e-10\n",
      "474 1.63399196950742e-10\n",
      "475 1.5714519963072604e-10\n",
      "476 1.5575911394005715e-10\n",
      "477 1.5213584558804172e-10\n",
      "478 1.486489958901771e-10\n",
      "479 1.4868262177003544e-10\n",
      "480 1.4343326526500277e-10\n",
      "481 1.418619666182508e-10\n",
      "482 1.3836495837971086e-10\n",
      "483 1.3478450300308253e-10\n",
      "484 1.3220946559755475e-10\n",
      "485 1.2617869249442748e-10\n",
      "486 1.2458942211246438e-10\n",
      "487 1.2281120564949788e-10\n",
      "488 1.200498866982258e-10\n",
      "489 1.1848770575806356e-10\n",
      "490 1.1719766823681255e-10\n",
      "491 1.141524375025682e-10\n",
      "492 1.1017821377468096e-10\n",
      "493 1.0934338157131407e-10\n",
      "494 1.0607520417593719e-10\n",
      "495 1.0619506662923328e-10\n",
      "496 1.0396786209732056e-10\n",
      "497 1.0362567748334328e-10\n",
      "498 1.0050185683674329e-10\n",
      "499 9.753821361702109e-11\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print (t, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2044,  0.0993, -0.2014,  0.1418,  0.4761, -0.3334, -0.5975,\n",
       "          0.1119,  0.1036,  1.6189],\n",
       "        [-0.0395, -0.1770, -0.3403, -0.3870,  1.3188,  0.7843, -1.8573,\n",
       "         -0.3582, -2.1852,  1.4687],\n",
       "        [ 0.2395,  2.1440,  0.7256, -0.3925, -0.4425, -0.2446,  0.5864,\n",
       "          0.0175,  0.4021,  0.5384],\n",
       "        [ 1.6520,  1.8150, -0.5895,  0.4817, -0.4521,  1.3234, -1.5491,\n",
       "         -0.1496,  0.6170,  1.6410],\n",
       "        [ 0.8639, -1.0313,  1.3064, -0.6706,  1.0874,  0.4264,  0.5151,\n",
       "          0.8027,  0.4105, -1.1921],\n",
       "        [-2.1385,  0.1517,  1.6480, -0.4272, -0.9012,  2.0525, -1.0993,\n",
       "          1.0511, -0.5071,  1.1091],\n",
       "        [-1.9430, -1.4017,  0.7424, -0.3892, -0.3394, -0.2491,  2.2769,\n",
       "          0.3689, -1.2169, -0.6053],\n",
       "        [ 0.6586, -1.3983,  0.3691,  1.3329,  0.1632,  0.7760,  0.5142,\n",
       "         -0.1493,  0.0359,  0.5288],\n",
       "        [-0.2754, -1.1055, -1.4322, -1.1166,  1.8896, -0.0907,  0.2977,\n",
       "         -1.4087, -0.3779, -0.7132],\n",
       "        [-0.7606,  0.5518, -1.2284, -0.5102,  0.5549, -0.3269,  0.9834,\n",
       "         -1.1245, -0.2217,  1.0153],\n",
       "        [-0.3239,  0.3647,  0.5662, -0.1830, -0.7695,  0.9020,  0.1260,\n",
       "          0.6378, -0.1474,  1.5806],\n",
       "        [ 0.0706,  3.1369, -1.0140,  2.7001,  0.3387, -1.5539,  1.1446,\n",
       "         -0.8470,  1.1193,  0.2025],\n",
       "        [-1.5494,  0.0387,  1.0542,  0.4909,  0.6523,  1.1848, -0.2128,\n",
       "         -0.5295,  1.4849,  0.5890],\n",
       "        [-0.7466, -0.0653, -1.8931,  1.8264,  0.7229, -0.2051, -2.4041,\n",
       "         -1.1468, -0.9638,  0.7010],\n",
       "        [ 0.1072,  0.0548,  1.3348,  0.9488,  0.2552,  0.7739,  0.6654,\n",
       "         -0.2575, -1.2920, -0.0846],\n",
       "        [-1.4573, -1.0501,  1.5692, -0.2579, -0.0060,  1.2365,  0.3886,\n",
       "         -0.6472,  0.4571,  0.7604],\n",
       "        [-0.6105, -1.6503,  0.5123,  1.1631, -0.6271, -0.6109,  0.5024,\n",
       "         -1.6178, -1.0913, -0.2555],\n",
       "        [ 1.0258, -0.6653, -0.1579,  0.0844, -1.7545,  1.0847, -1.2899,\n",
       "          0.2048,  0.4677,  0.3974],\n",
       "        [-0.3727, -0.7286, -1.0747,  0.0147, -0.6282,  2.5253,  1.1556,\n",
       "         -1.1288, -2.1391, -0.3454],\n",
       "        [-0.6621,  1.2848,  1.4793, -1.0853,  0.6904, -0.4724, -0.0037,\n",
       "         -0.9009, -0.1728,  2.6640],\n",
       "        [ 0.0484,  0.7337,  1.4868,  0.5113, -0.6208, -0.3973,  1.1470,\n",
       "         -0.4712, -1.1588, -0.4216],\n",
       "        [-0.1955,  1.8742,  0.0056,  0.1021, -0.8698, -1.8004,  2.0224,\n",
       "         -0.6608,  0.6922, -1.7458],\n",
       "        [-1.5261, -0.6930,  0.2515, -0.4544, -0.2841, -0.9211, -0.5579,\n",
       "          0.8853,  1.2729,  1.3037],\n",
       "        [-0.5826, -0.2565, -1.1291, -1.6495, -0.3098,  0.7032, -0.0361,\n",
       "         -1.1748, -0.6925, -1.5773],\n",
       "        [ 0.2235,  0.4587,  1.2963, -0.4805, -1.4632,  0.1743,  0.0497,\n",
       "         -0.2163, -0.6375, -0.4807],\n",
       "        [ 1.7585, -0.7946,  1.4950,  0.2488,  0.7645, -0.3008, -1.3770,\n",
       "         -0.1742, -0.2547, -1.5215],\n",
       "        [ 0.7469, -2.1280, -0.4323,  0.2830, -0.6227, -0.6108, -0.5121,\n",
       "         -1.5211,  0.5485,  0.6194],\n",
       "        [-0.6697,  0.4345,  0.3994,  1.0767,  0.2105, -0.5916,  0.5884,\n",
       "         -1.1006, -0.2774, -0.0499],\n",
       "        [ 2.4387, -0.6732,  0.3240,  1.5491, -0.2220, -0.2320, -0.9985,\n",
       "         -0.4778,  0.1375,  0.4181],\n",
       "        [ 0.5445, -1.1180,  0.1054, -1.0158,  1.5416, -0.3102,  1.6745,\n",
       "          1.1531,  0.6664, -0.6898],\n",
       "        [-1.3278,  1.7694,  0.6456, -1.1018, -1.2468,  0.7250,  1.4089,\n",
       "          0.3246, -2.7825,  0.4342],\n",
       "        [-0.4600,  0.3056, -0.5678,  1.3640,  0.3760, -1.7705, -1.5917,\n",
       "         -0.6298, -0.5592,  0.4316],\n",
       "        [-0.5944, -1.1291, -1.2124,  0.7668,  0.3194,  0.3395,  0.2171,\n",
       "         -0.7015,  1.7603,  0.4255],\n",
       "        [-0.7201, -0.1575,  0.2042, -0.8552,  0.4132,  0.6129, -0.1014,\n",
       "         -1.7361,  0.7970, -0.0327],\n",
       "        [-1.2686, -0.6905,  0.6684, -1.4454, -0.2906,  0.4200,  0.8150,\n",
       "          0.8673, -1.2063, -0.0230],\n",
       "        [ 0.8344, -0.4689,  0.0748,  1.0507, -0.3053,  0.0013, -1.0870,\n",
       "         -0.0595,  1.4467,  0.2139],\n",
       "        [ 1.3132, -0.5900,  1.3416, -0.2285,  0.2902,  0.5697, -0.6865,\n",
       "          1.0540,  0.5102, -0.6393],\n",
       "        [ 2.2594,  1.2673, -0.6949, -0.9121,  0.7351,  1.9553, -0.4511,\n",
       "         -0.4766, -1.0981,  2.2143],\n",
       "        [ 0.0923, -0.7497,  0.8397,  1.0521, -2.4716,  0.7438,  1.0812,\n",
       "         -0.8540, -0.2923,  0.7032],\n",
       "        [-2.6546,  0.1249, -0.6467,  1.4578,  0.2812, -0.7546,  0.6352,\n",
       "         -1.1743,  0.2109, -0.9698],\n",
       "        [ 0.3854,  0.6472, -3.0933, -1.6553, -1.3872,  0.9784, -2.0249,\n",
       "          0.1321,  2.0636,  0.2260],\n",
       "        [ 0.6346,  0.5123,  0.8568,  0.5763,  0.1054,  2.6419,  0.1749,\n",
       "          1.2243, -1.7606,  0.5648],\n",
       "        [ 1.4075,  0.1444,  0.5380,  0.8365,  1.7643, -0.2879, -1.6838,\n",
       "          0.3220,  0.4220, -0.4057],\n",
       "        [-0.1122, -1.8162, -0.6124,  1.3397, -1.3446,  0.2130,  0.4880,\n",
       "          0.2664, -0.9288, -0.8348],\n",
       "        [-0.7852, -1.9737,  0.8755, -0.7125, -0.2408, -0.7374,  0.4553,\n",
       "          0.5048, -0.9646,  1.9069],\n",
       "        [-0.2811,  1.8922, -0.1885,  3.0931,  0.3650,  1.4426,  0.5955,\n",
       "         -0.5997,  1.9755, -0.9189],\n",
       "        [ 1.0607, -0.4425, -1.2047, -1.4414, -0.6884, -1.6227, -0.0934,\n",
       "          1.4554,  1.7326,  0.9380],\n",
       "        [ 0.6513,  2.9901,  1.7853, -0.6100, -0.4346, -0.8218,  2.0277,\n",
       "          0.0185,  0.7305, -1.2540],\n",
       "        [ 1.2015, -1.2062, -0.0368,  0.5567,  1.2316, -1.3628,  1.5848,\n",
       "          0.8455,  0.1692,  2.0957],\n",
       "        [-0.2789, -0.7373,  1.4246, -1.5983, -0.1554, -0.7064,  0.1300,\n",
       "          0.1088,  0.7735,  0.3068],\n",
       "        [ 0.6619, -0.4753,  1.5129,  0.1158,  0.0522,  2.8241,  0.7079,\n",
       "          0.1365, -0.1168,  0.6907],\n",
       "        [-0.7708,  0.8831,  0.8626,  0.6424,  1.4565,  0.6764, -0.3161,\n",
       "          0.7484,  0.1338,  1.4635],\n",
       "        [-1.5147,  0.6893,  0.6838, -0.2102, -3.3723,  0.6421,  0.9790,\n",
       "         -0.6870,  0.3250,  0.4263],\n",
       "        [-0.3692, -0.5400, -1.2339, -0.2575, -0.1894,  1.0620,  1.2141,\n",
       "          0.7783, -0.2893, -0.1055],\n",
       "        [ 0.3838, -0.1741,  0.9711, -1.3745, -0.1299, -0.2473,  1.0127,\n",
       "          0.2807, -0.0825, -0.0067],\n",
       "        [-0.2375, -0.2909, -0.0166,  1.3349, -1.2781, -0.8226,  0.0129,\n",
       "         -0.4120,  0.9033,  1.8704],\n",
       "        [ 0.9541,  1.3907, -0.0818,  0.7541,  0.2943,  0.4737, -0.7580,\n",
       "         -0.9980,  1.2280,  0.7069],\n",
       "        [ 1.1019,  1.0189, -1.0178,  0.6039,  0.4256,  0.0399,  1.3082,\n",
       "         -0.1447, -0.6447,  0.5043],\n",
       "        [ 1.0397,  0.2873, -0.1174,  0.4807,  0.0542,  1.3108,  2.0734,\n",
       "          0.5551, -0.1882,  0.1443],\n",
       "        [-0.6074,  0.0845, -0.4656,  0.2715, -0.7113, -0.8000, -0.5438,\n",
       "         -0.9116,  0.8896,  1.7390],\n",
       "        [-1.1385,  0.1609, -1.2484, -0.5178, -1.6466, -0.0851,  0.9458,\n",
       "          1.1874, -0.4460, -2.0393],\n",
       "        [ 0.6209,  0.4621, -1.7608,  0.7804,  0.2458, -1.3465, -0.9638,\n",
       "          1.5451, -0.2030, -1.4290],\n",
       "        [-0.6865,  0.4202, -1.1437, -0.9225, -0.8425, -0.0602, -0.7588,\n",
       "         -0.0662,  0.5329,  0.7556],\n",
       "        [ 0.9387,  0.2935, -0.2980, -0.9493, -0.7530, -1.4024, -1.3805,\n",
       "         -0.1372,  0.2871,  1.0132]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3026e-01,  4.1061e-01,  1.3140e+00,  ..., -1.8070e-01,\n",
       "         -1.6921e+00, -3.3212e-01],\n",
       "        [ 1.8450e-01, -1.0013e+00,  7.1709e-01,  ..., -2.8718e-01,\n",
       "         -2.3109e-01, -1.3662e+00],\n",
       "        [-1.1361e+00, -6.5719e-01, -8.7136e-01,  ..., -1.3898e+00,\n",
       "         -5.8069e-01, -5.3959e-02],\n",
       "        ...,\n",
       "        [-3.3729e-01, -1.0465e+00,  3.4647e-01,  ...,  6.0241e-01,\n",
       "          7.3158e-01,  8.4829e-01],\n",
       "        [-4.0678e-01,  1.4929e+00,  1.0016e+00,  ..., -2.6611e-01,\n",
       "          8.1009e-02, -3.5511e-01],\n",
       "        [-8.1996e-01,  1.2899e+00, -3.4087e-01,  ..., -1.3661e+00,\n",
       "          6.9666e-01,  5.6599e-01]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1000])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, h, output_dim):\n",
    "        super(NN,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, h)\n",
    "        self.linear2 = torch.nn.Linear(h, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        output = self.linear2(h_relu)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = NN(input_dim, H, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n",
      "1.056598424911499\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    y_ = d(x)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer.zero_grad()  # Gradient = 0\n",
    "    loss = loss_fn(y_, y)\n",
    "    print (loss.item())\n",
    "    loss.backward() # Backprop\n",
    "    optimizer.step() # Updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.2263e-01,  7.0465e-01,  3.8445e-01,  ...,  1.1973e+00,\n",
       "         -1.5793e+00,  5.7533e-01],\n",
       "        [-5.7792e-01,  2.9865e-01,  4.5245e-01,  ..., -1.0411e-01,\n",
       "         -5.0436e-01,  6.3357e-01],\n",
       "        [ 3.7090e-01, -4.1821e-01,  7.0710e-01,  ...,  3.8338e-01,\n",
       "          1.6227e-01,  2.8721e-01],\n",
       "        ...,\n",
       "        [-1.0006e+00, -3.7468e-02,  9.7670e-01,  ..., -1.0902e+00,\n",
       "          4.3084e-01, -5.4180e-02],\n",
       "        [-4.7938e-01,  5.4371e-01, -4.9878e-02,  ..., -7.4853e-01,\n",
       "         -7.5786e-01,  1.5172e-01],\n",
       "        [ 9.4257e-01,  1.0826e+00, -6.7559e-01,  ..., -1.0472e+00,\n",
       "         -1.2825e-01, -6.2468e-01]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.nn.ReLU()\n",
    "c = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(torch.ones(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_classification(n_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[0]\n",
    "labels = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(features.shape[1],100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100,74),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(74, 2),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=5, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=74, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=74, out_features=2, bias=True)\n",
       "  (5): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "pred = model(torch.from_numpy(features).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.34348297119140625\n",
      "1 0.3434823155403137\n",
      "2 0.3434818685054779\n",
      "3 0.34348130226135254\n",
      "4 0.34348064661026\n",
      "5 0.34348008036613464\n",
      "6 0.3434794247150421\n",
      "7 0.3434787094593048\n",
      "8 0.3434782028198242\n",
      "9 0.34347766637802124\n",
      "10 0.3434771001338959\n",
      "11 0.34347644448280334\n",
      "12 0.34347590804100037\n",
      "13 0.34347546100616455\n",
      "14 0.343474805355072\n",
      "15 0.34347423911094666\n",
      "16 0.34347379207611084\n",
      "17 0.34347325563430786\n",
      "18 0.34347274899482727\n",
      "19 0.34347209334373474\n",
      "20 0.34347155690193176\n",
      "21 0.3434709906578064\n",
      "22 0.3434704840183258\n",
      "23 0.3434700071811676\n",
      "24 0.34346938133239746\n",
      "25 0.34346893429756165\n",
      "26 0.34346842765808105\n",
      "27 0.3434678912162781\n",
      "28 0.3434674143791199\n",
      "29 0.3434669077396393\n",
      "30 0.3434663414955139\n",
      "31 0.3434658348560333\n",
      "32 0.3434653878211975\n",
      "33 0.34346482157707214\n",
      "34 0.34346428513526917\n",
      "35 0.3434637188911438\n",
      "36 0.3434630334377289\n",
      "37 0.34346267580986023\n",
      "38 0.3434622287750244\n",
      "39 0.3434616029262543\n",
      "40 0.34346118569374084\n",
      "41 0.3434606194496155\n",
      "42 0.3434600830078125\n",
      "43 0.3434596359729767\n",
      "44 0.34345921874046326\n",
      "45 0.34345871210098267\n",
      "46 0.3434581458568573\n",
      "47 0.34345772862434387\n",
      "48 0.3434571325778961\n",
      "49 0.34345677495002747\n",
      "50 0.3434563875198364\n",
      "51 0.34345585107803345\n",
      "52 0.3434552252292633\n",
      "53 0.3434548079967499\n",
      "54 0.34345436096191406\n",
      "55 0.3434538245201111\n",
      "56 0.3434532880783081\n",
      "57 0.3434528708457947\n",
      "58 0.3434523642063141\n",
      "59 0.34345194697380066\n",
      "60 0.3434514105319977\n",
      "61 0.34345096349716187\n",
      "62 0.3434505760669708\n",
      "63 0.34345000982284546\n",
      "64 0.34344956278800964\n",
      "65 0.3434491455554962\n",
      "66 0.3434487581253052\n",
      "67 0.343448281288147\n",
      "68 0.34344783425331116\n",
      "69 0.3434472680091858\n",
      "70 0.34344688057899475\n",
      "71 0.3434464931488037\n",
      "72 0.34344592690467834\n",
      "73 0.3434455096721649\n",
      "74 0.34344521164894104\n",
      "75 0.34344473481178284\n",
      "76 0.343444287776947\n",
      "77 0.3434438407421112\n",
      "78 0.3434433043003082\n",
      "79 0.3434429466724396\n",
      "80 0.34344249963760376\n",
      "81 0.34344205260276794\n",
      "82 0.3434416651725769\n",
      "83 0.3434412479400635\n",
      "84 0.3434406518936157\n",
      "85 0.34344035387039185\n",
      "86 0.34344005584716797\n",
      "87 0.3434394896030426\n",
      "88 0.343438982963562\n",
      "89 0.34343868494033813\n",
      "90 0.3434382677078247\n",
      "91 0.3434378504753113\n",
      "92 0.3434373736381531\n",
      "93 0.3434368968009949\n",
      "94 0.3434365391731262\n",
      "95 0.3434361517429352\n",
      "96 0.34343570470809937\n",
      "97 0.34343528747558594\n",
      "98 0.3434349000453949\n",
      "99 0.3434344232082367\n",
      "100 0.34343409538269043\n",
      "101 0.3434336483478546\n",
      "102 0.34343311190605164\n",
      "103 0.34343278408050537\n",
      "104 0.34343239665031433\n",
      "105 0.3434319794178009\n",
      "106 0.3434315025806427\n",
      "107 0.34343117475509644\n",
      "108 0.343430757522583\n",
      "109 0.34343042969703674\n",
      "110 0.3434300720691681\n",
      "111 0.3434295952320099\n",
      "112 0.34342917799949646\n",
      "113 0.3434288799762726\n",
      "114 0.34342846274375916\n",
      "115 0.34342801570892334\n",
      "116 0.3434276580810547\n",
      "117 0.3434273600578308\n",
      "118 0.34342706203460693\n",
      "119 0.3434267044067383\n",
      "120 0.3434261381626129\n",
      "121 0.3434257507324219\n",
      "122 0.3434255123138428\n",
      "123 0.3434251844882965\n",
      "124 0.34342482686042786\n",
      "125 0.34342437982559204\n",
      "126 0.34342384338378906\n",
      "127 0.3434235453605652\n",
      "128 0.3434232771396637\n",
      "129 0.34342291951179504\n",
      "130 0.3434225022792816\n",
      "131 0.3434221148490906\n",
      "132 0.34342169761657715\n",
      "133 0.3434212803840637\n",
      "134 0.34342095255851746\n",
      "135 0.3434206247329712\n",
      "136 0.34342020750045776\n",
      "137 0.34341979026794434\n",
      "138 0.34341946244239807\n",
      "139 0.34341922402381897\n",
      "140 0.3434188961982727\n",
      "141 0.34341850876808167\n",
      "142 0.343418151140213\n",
      "143 0.343417763710022\n",
      "144 0.34341731667518616\n",
      "145 0.3434169888496399\n",
      "146 0.3434167802333832\n",
      "147 0.3434164822101593\n",
      "148 0.3434161841869354\n",
      "149 0.3434157073497772\n",
      "150 0.3434152901172638\n",
      "151 0.3434149920940399\n",
      "152 0.3434147238731384\n",
      "153 0.34341439604759216\n",
      "154 0.3434140086174011\n",
      "155 0.34341365098953247\n",
      "156 0.34341323375701904\n",
      "157 0.34341296553611755\n",
      "158 0.3434126675128937\n",
      "159 0.34341225028038025\n",
      "160 0.3434118926525116\n",
      "161 0.3434115946292877\n",
      "162 0.34341129660606384\n",
      "163 0.3434109389781952\n",
      "164 0.34341058135032654\n",
      "165 0.3434102535247803\n",
      "166 0.343409925699234\n",
      "167 0.34340953826904297\n",
      "168 0.3434091806411743\n",
      "169 0.34340888261795044\n",
      "170 0.3434085547924042\n",
      "171 0.3434081971645355\n",
      "172 0.34340789914131165\n",
      "173 0.34340760111808777\n",
      "174 0.3434072434902191\n",
      "175 0.3434069752693176\n",
      "176 0.34340670704841614\n",
      "177 0.3434062898159027\n",
      "178 0.34340599179267883\n",
      "179 0.34340569376945496\n",
      "180 0.3434053361415863\n",
      "181 0.343405157327652\n",
      "182 0.3434047996997833\n",
      "183 0.3434044122695923\n",
      "184 0.34340405464172363\n",
      "185 0.34340378642082214\n",
      "186 0.3434034585952759\n",
      "187 0.343403160572052\n",
      "188 0.3434028625488281\n",
      "189 0.34340259432792664\n",
      "190 0.34340226650238037\n",
      "191 0.3434019088745117\n",
      "192 0.3434017300605774\n",
      "193 0.3434014618396759\n",
      "194 0.34340113401412964\n",
      "195 0.343400776386261\n",
      "196 0.3434005081653595\n",
      "197 0.343400239944458\n",
      "198 0.3433999717235565\n",
      "199 0.34339961409568787\n",
      "200 0.343399316072464\n",
      "201 0.3433990776538849\n",
      "202 0.3433987498283386\n",
      "203 0.3433985412120819\n",
      "204 0.3433982729911804\n",
      "205 0.34339794516563416\n",
      "206 0.34339749813079834\n",
      "207 0.34339725971221924\n",
      "208 0.34339699149131775\n",
      "209 0.34339669346809387\n",
      "210 0.3433964252471924\n",
      "211 0.3433960974216461\n",
      "212 0.34339579939842224\n",
      "213 0.34339550137519836\n",
      "214 0.34339526295661926\n",
      "215 0.3433949947357178\n",
      "216 0.3433946669101715\n",
      "217 0.34339439868927\n",
      "218 0.3433941602706909\n",
      "219 0.3433939218521118\n",
      "220 0.34339359402656555\n",
      "221 0.3433932960033417\n",
      "222 0.34339290857315063\n",
      "223 0.3433927595615387\n",
      "224 0.3433924615383148\n",
      "225 0.34339213371276855\n",
      "226 0.34339186549186707\n",
      "227 0.3433915376663208\n",
      "228 0.34339138865470886\n",
      "229 0.3433910608291626\n",
      "230 0.3433908224105835\n",
      "231 0.34339049458503723\n",
      "232 0.34339019656181335\n",
      "233 0.34339001774787903\n",
      "234 0.3433898091316223\n",
      "235 0.34338948130607605\n",
      "236 0.3433891534805298\n",
      "237 0.34338894486427307\n",
      "238 0.34338870644569397\n",
      "239 0.3433884382247925\n",
      "240 0.3433881402015686\n",
      "241 0.3433879017829895\n",
      "242 0.3433876037597656\n",
      "243 0.34338733553886414\n",
      "244 0.34338706731796265\n",
      "245 0.3433868885040283\n",
      "246 0.34338656067848206\n",
      "247 0.3433862626552582\n",
      "248 0.3433859944343567\n",
      "249 0.3433857858181\n",
      "250 0.3433855473995209\n",
      "251 0.3433853089809418\n",
      "252 0.3433849811553955\n",
      "253 0.343384712934494\n",
      "254 0.3433845043182373\n",
      "255 0.3433842957019806\n",
      "256 0.3433839678764343\n",
      "257 0.3433837592601776\n",
      "258 0.3433835208415985\n",
      "259 0.34338322281837463\n",
      "260 0.34338292479515076\n",
      "261 0.34338274598121643\n",
      "262 0.3433825969696045\n",
      "263 0.3433822989463806\n",
      "264 0.34338197112083435\n",
      "265 0.3433817923069\n",
      "266 0.3433816134929657\n",
      "267 0.3433813750743866\n",
      "268 0.34338104724884033\n",
      "269 0.34338074922561646\n",
      "270 0.34338054060935974\n",
      "271 0.343380331993103\n",
      "272 0.3433799743652344\n",
      "273 0.3433798551559448\n",
      "274 0.3433796167373657\n",
      "275 0.343379408121109\n",
      "276 0.3433791697025299\n",
      "277 0.3433789312839508\n",
      "278 0.3433787226676941\n",
      "279 0.3433784544467926\n",
      "280 0.3433782160282135\n",
      "281 0.3433780372142792\n",
      "282 0.3433777093887329\n",
      "283 0.34337756037712097\n",
      "284 0.34337738156318665\n",
      "285 0.34337714314460754\n",
      "286 0.34337687492370605\n",
      "287 0.3433765769004822\n",
      "288 0.3433764576911926\n",
      "289 0.3433762788772583\n",
      "290 0.3433760702610016\n",
      "291 0.3433757424354553\n",
      "292 0.34337544441223145\n",
      "293 0.34337520599365234\n",
      "294 0.343375027179718\n",
      "295 0.3433747887611389\n",
      "296 0.3433745503425598\n",
      "297 0.3433742821216583\n",
      "298 0.343374103307724\n",
      "299 0.3433738946914673\n",
      "300 0.34337368607521057\n",
      "301 0.34337344765663147\n",
      "302 0.34337320923805237\n",
      "303 0.34337300062179565\n",
      "304 0.34337276220321655\n",
      "305 0.3433725833892822\n",
      "306 0.34337231516838074\n",
      "307 0.343372106552124\n",
      "308 0.34337183833122253\n",
      "309 0.3433716893196106\n",
      "310 0.3433714807033539\n",
      "311 0.3433712422847748\n",
      "312 0.34337103366851807\n",
      "313 0.34337085485458374\n",
      "314 0.34337061643600464\n",
      "315 0.3433704078197479\n",
      "316 0.343370258808136\n",
      "317 0.3433700501918793\n",
      "318 0.34336987137794495\n",
      "319 0.34336957335472107\n",
      "320 0.3433694541454315\n",
      "321 0.3433693051338196\n",
      "322 0.3433690667152405\n",
      "323 0.343368798494339\n",
      "324 0.3433685302734375\n",
      "325 0.34336838126182556\n",
      "326 0.34336817264556885\n",
      "327 0.3433680236339569\n",
      "328 0.3433677554130554\n",
      "329 0.3433675765991211\n",
      "330 0.343367338180542\n",
      "331 0.34336715936660767\n",
      "332 0.3433670103549957\n",
      "333 0.3433668613433838\n",
      "334 0.34336671233177185\n",
      "335 0.34336647391319275\n",
      "336 0.3433660864830017\n",
      "337 0.34336593747138977\n",
      "338 0.3433658182621002\n",
      "339 0.3433655798435211\n",
      "340 0.34336531162261963\n",
      "341 0.3433651626110077\n",
      "342 0.343364953994751\n",
      "343 0.34336474537849426\n",
      "344 0.34336456656455994\n",
      "345 0.343364417552948\n",
      "346 0.3433641791343689\n",
      "347 0.34336400032043457\n",
      "348 0.34336376190185547\n",
      "349 0.34336358308792114\n",
      "350 0.3433634042739868\n",
      "351 0.3433632254600525\n",
      "352 0.34336304664611816\n",
      "353 0.34336280822753906\n",
      "354 0.34336256980895996\n",
      "355 0.3433624505996704\n",
      "356 0.34336230158805847\n",
      "357 0.34336209297180176\n",
      "358 0.34336188435554504\n",
      "359 0.3433617055416107\n",
      "360 0.3433614671230316\n",
      "361 0.3433613181114197\n",
      "362 0.34336113929748535\n",
      "363 0.34336093068122864\n",
      "364 0.3433608114719391\n",
      "365 0.34336063265800476\n",
      "366 0.34336036443710327\n",
      "367 0.34336021542549133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 0.34335997700691223\n",
      "369 0.3433598279953003\n",
      "370 0.34335964918136597\n",
      "371 0.34335950016975403\n",
      "372 0.3433593511581421\n",
      "373 0.3433590829372406\n",
      "374 0.3433590233325958\n",
      "375 0.34335875511169434\n",
      "376 0.3433585464954376\n",
      "377 0.3433583378791809\n",
      "378 0.34335818886756897\n",
      "379 0.34335801005363464\n",
      "380 0.34335780143737793\n",
      "381 0.343357652425766\n",
      "382 0.34335747361183167\n",
      "383 0.3433573544025421\n",
      "384 0.3433571755886078\n",
      "385 0.3433569669723511\n",
      "386 0.343356728553772\n",
      "387 0.3433566689491272\n",
      "388 0.34335651993751526\n",
      "389 0.34335631132125854\n",
      "390 0.34335610270500183\n",
      "391 0.3433559536933899\n",
      "392 0.34335583448410034\n",
      "393 0.34335562586784363\n",
      "394 0.3433554172515869\n",
      "395 0.343355268239975\n",
      "396 0.34335505962371826\n",
      "397 0.34335488080978394\n",
      "398 0.343354731798172\n",
      "399 0.34335458278656006\n",
      "400 0.34335437417030334\n",
      "401 0.3433542251586914\n",
      "402 0.34335407614707947\n",
      "403 0.34335392713546753\n",
      "404 0.3433537185192108\n",
      "405 0.3433535695075989\n",
      "406 0.34335342049598694\n",
      "407 0.34335318207740784\n",
      "408 0.3433530330657959\n",
      "409 0.34335291385650635\n",
      "410 0.343352735042572\n",
      "411 0.3433525860309601\n",
      "412 0.34335246682167053\n",
      "413 0.3433522880077362\n",
      "414 0.3433520495891571\n",
      "415 0.34335193037986755\n",
      "416 0.3433517515659332\n",
      "417 0.3433516323566437\n",
      "418 0.34335145354270935\n",
      "419 0.34335124492645264\n",
      "420 0.3433511257171631\n",
      "421 0.34335097670555115\n",
      "422 0.3433508574962616\n",
      "423 0.34335067868232727\n",
      "424 0.34335049986839294\n",
      "425 0.34335029125213623\n",
      "426 0.34335026144981384\n",
      "427 0.34335005283355713\n",
      "428 0.3433498740196228\n",
      "429 0.34334975481033325\n",
      "430 0.34334948658943176\n",
      "431 0.343349426984787\n",
      "432 0.34334927797317505\n",
      "433 0.3433490991592407\n",
      "434 0.34334900975227356\n",
      "435 0.34334880113601685\n",
      "436 0.34334859251976013\n",
      "437 0.3433484733104706\n",
      "438 0.34334835410118103\n",
      "439 0.3433482050895691\n",
      "440 0.34334796667099\n",
      "441 0.34334781765937805\n",
      "442 0.3433476984500885\n",
      "443 0.34334754943847656\n",
      "444 0.34334737062454224\n",
      "445 0.3433472514152527\n",
      "446 0.34334704279899597\n",
      "447 0.3433469533920288\n",
      "448 0.3433467745780945\n",
      "449 0.34334662556648254\n",
      "450 0.3433464765548706\n",
      "451 0.34334632754325867\n",
      "452 0.3433462083339691\n",
      "453 0.3433460295200348\n",
      "454 0.34334591031074524\n",
      "455 0.3433457911014557\n",
      "456 0.34334564208984375\n",
      "457 0.3433454930782318\n",
      "458 0.3433452844619751\n",
      "459 0.34334519505500793\n",
      "460 0.343345046043396\n",
      "461 0.34334486722946167\n",
      "462 0.34334471821784973\n",
      "463 0.3433445990085602\n",
      "464 0.34334444999694824\n",
      "465 0.3433442711830139\n",
      "466 0.34334415197372437\n",
      "467 0.3433440327644348\n",
      "468 0.3433438837528229\n",
      "469 0.3433437645435333\n",
      "470 0.3433436453342438\n",
      "471 0.3433435559272766\n",
      "472 0.3433433473110199\n",
      "473 0.34334316849708557\n",
      "474 0.34334301948547363\n",
      "475 0.3433428704738617\n",
      "476 0.34334275126457214\n",
      "477 0.3433426320552826\n",
      "478 0.34334248304367065\n",
      "479 0.3433423340320587\n",
      "480 0.34334221482276917\n",
      "481 0.3433420658111572\n",
      "482 0.3433419167995453\n",
      "483 0.34334179759025574\n",
      "484 0.3433417081832886\n",
      "485 0.34334155917167664\n",
      "486 0.3433413803577423\n",
      "487 0.34334129095077515\n",
      "488 0.3433411419391632\n",
      "489 0.34334102272987366\n",
      "490 0.3433409035205841\n",
      "491 0.34334075450897217\n",
      "492 0.34334060549736023\n",
      "493 0.3433404862880707\n",
      "494 0.34334036707878113\n",
      "495 0.3433401882648468\n",
      "496 0.34334003925323486\n",
      "497 0.3433399200439453\n",
      "498 0.3433397710323334\n",
      "499 0.3433396518230438\n",
      "500 0.3433395326137543\n",
      "501 0.34333938360214233\n",
      "502 0.3433392643928528\n",
      "503 0.34333911538124084\n",
      "504 0.34333905577659607\n",
      "505 0.3433389663696289\n",
      "506 0.34333881735801697\n",
      "507 0.34333860874176025\n",
      "508 0.3433384597301483\n",
      "509 0.34333840012550354\n",
      "510 0.3433383107185364\n",
      "511 0.34333816170692444\n",
      "512 0.3433379828929901\n",
      "513 0.34333786368370056\n",
      "514 0.343337744474411\n",
      "515 0.34333762526512146\n",
      "516 0.3433374762535095\n",
      "517 0.3433373272418976\n",
      "518 0.34333720803260803\n",
      "519 0.3433370590209961\n",
      "520 0.34333693981170654\n",
      "521 0.34333691000938416\n",
      "522 0.3433367908000946\n",
      "523 0.34333664178848267\n",
      "524 0.34333646297454834\n",
      "525 0.343336284160614\n",
      "526 0.3433362543582916\n",
      "527 0.34333619475364685\n",
      "528 0.3433360159397125\n",
      "529 0.34333592653274536\n",
      "530 0.34333571791648865\n",
      "531 0.3433355987071991\n",
      "532 0.34333550930023193\n",
      "533 0.34333541989326477\n",
      "534 0.3433353006839752\n",
      "535 0.34333518147468567\n",
      "536 0.3433350622653961\n",
      "537 0.3433349132537842\n",
      "538 0.34333473443984985\n",
      "539 0.3433346450328827\n",
      "540 0.3433345556259155\n",
      "541 0.34333446621894836\n",
      "542 0.34333428740501404\n",
      "543 0.3433341681957245\n",
      "544 0.34333404898643494\n",
      "545 0.3433339595794678\n",
      "546 0.34333381056785583\n",
      "547 0.3433336913585663\n",
      "548 0.34333354234695435\n",
      "549 0.34333348274230957\n",
      "550 0.34333336353302\n",
      "551 0.3433332145214081\n",
      "552 0.3433331251144409\n",
      "553 0.343332976102829\n",
      "554 0.34333285689353943\n",
      "555 0.3433327376842499\n",
      "556 0.3433326482772827\n",
      "557 0.34333252906799316\n",
      "558 0.3433324098587036\n",
      "559 0.34333232045173645\n",
      "560 0.3433322012424469\n",
      "561 0.34333211183547974\n",
      "562 0.3433319330215454\n",
      "563 0.343331903219223\n",
      "564 0.3433317542076111\n",
      "565 0.34333163499832153\n",
      "566 0.34333157539367676\n",
      "567 0.3433314561843872\n",
      "568 0.3433312475681305\n",
      "569 0.34333115816116333\n",
      "570 0.3433310389518738\n",
      "571 0.34333091974258423\n",
      "572 0.3433307707309723\n",
      "573 0.3433306813240051\n",
      "574 0.3433305621147156\n",
      "575 0.3433304727077484\n",
      "576 0.34333035349845886\n",
      "577 0.3433302342891693\n",
      "578 0.34333014488220215\n",
      "579 0.3433299958705902\n",
      "580 0.34332987666130066\n",
      "581 0.3433297872543335\n",
      "582 0.34332969784736633\n",
      "583 0.3433295786380768\n",
      "584 0.34332942962646484\n",
      "585 0.3433293402194977\n",
      "586 0.3433292508125305\n",
      "587 0.34332913160324097\n",
      "588 0.3433290719985962\n",
      "589 0.34332892298698425\n",
      "590 0.3433288335800171\n",
      "591 0.34332871437072754\n",
      "592 0.343328595161438\n",
      "593 0.34332847595214844\n",
      "594 0.3433283567428589\n",
      "595 0.3433282971382141\n",
      "596 0.34332817792892456\n",
      "597 0.343328058719635\n",
      "598 0.34332793951034546\n",
      "599 0.3433278799057007\n",
      "600 0.3433277904987335\n",
      "601 0.34332770109176636\n",
      "602 0.3433275520801544\n",
      "603 0.34332743287086487\n",
      "604 0.34332728385925293\n",
      "605 0.34332725405693054\n",
      "606 0.3433271646499634\n",
      "607 0.3433270752429962\n",
      "608 0.34332695603370667\n",
      "609 0.3433268368244171\n",
      "610 0.34332671761512756\n",
      "611 0.3433266580104828\n",
      "612 0.3433265686035156\n",
      "613 0.3433264195919037\n",
      "614 0.3433263301849365\n",
      "615 0.34332627058029175\n",
      "616 0.3433261811733246\n",
      "617 0.34332603216171265\n",
      "618 0.3433259129524231\n",
      "619 0.34332579374313354\n",
      "620 0.34332573413848877\n",
      "621 0.3433256447315216\n",
      "622 0.34332552552223206\n",
      "623 0.3433254361152649\n",
      "624 0.34332531690597534\n",
      "625 0.3433252274990082\n",
      "626 0.34332507848739624\n",
      "627 0.34332501888275146\n",
      "628 0.3433249592781067\n",
      "629 0.34332484006881714\n",
      "630 0.34332475066185\n",
      "631 0.3433246314525604\n",
      "632 0.34332454204559326\n",
      "633 0.3433244824409485\n",
      "634 0.3433243930339813\n",
      "635 0.3433242738246918\n",
      "636 0.34332412481307983\n",
      "637 0.34332406520843506\n",
      "638 0.3433240056037903\n",
      "639 0.34332388639450073\n",
      "640 0.34332382678985596\n",
      "641 0.3433237075805664\n",
      "642 0.34332358837127686\n",
      "643 0.34332355856895447\n",
      "644 0.3433234393596649\n",
      "645 0.34332334995269775\n",
      "646 0.3433232009410858\n",
      "647 0.3433231711387634\n",
      "648 0.3433230519294739\n",
      "649 0.3433229923248291\n",
      "650 0.34332290291786194\n",
      "651 0.3433227837085724\n",
      "652 0.3433227241039276\n",
      "653 0.34332263469696045\n",
      "654 0.3433225154876709\n",
      "655 0.34332239627838135\n",
      "656 0.3433223366737366\n",
      "657 0.343322217464447\n",
      "658 0.34332209825515747\n",
      "659 0.3433219790458679\n",
      "660 0.34332191944122314\n",
      "661 0.3433218002319336\n",
      "662 0.3433217704296112\n",
      "663 0.34332165122032166\n",
      "664 0.3433215320110321\n",
      "665 0.34332144260406494\n",
      "666 0.34332138299942017\n",
      "667 0.3433212637901306\n",
      "668 0.34332120418548584\n",
      "669 0.3433211147785187\n",
      "670 0.3433209955692291\n",
      "671 0.34332093596458435\n",
      "672 0.3433208465576172\n",
      "673 0.34332075715065\n",
      "674 0.34332066774368286\n",
      "675 0.3433205783367157\n",
      "676 0.34332048892974854\n",
      "677 0.34332039952278137\n",
      "678 0.3433203101158142\n",
      "679 0.34332025051116943\n",
      "680 0.34332016110420227\n",
      "681 0.3433200418949127\n",
      "682 0.34332001209259033\n",
      "683 0.3433198928833008\n",
      "684 0.3433198034763336\n",
      "685 0.34331971406936646\n",
      "686 0.3433196544647217\n",
      "687 0.34331953525543213\n",
      "688 0.34331944584846497\n",
      "689 0.3433193564414978\n",
      "690 0.343319296836853\n",
      "691 0.34331920742988586\n",
      "692 0.3433191180229187\n",
      "693 0.34331896901130676\n",
      "694 0.343318909406662\n",
      "695 0.3433188199996948\n",
      "696 0.34331876039505005\n",
      "697 0.3433186709880829\n",
      "698 0.34331855177879333\n",
      "699 0.34331849217414856\n",
      "700 0.3433184027671814\n",
      "701 0.34331828355789185\n",
      "702 0.34331822395324707\n",
      "703 0.3433181345462799\n",
      "704 0.34331807494163513\n",
      "705 0.34331798553466797\n",
      "706 0.3433178961277008\n",
      "707 0.34331783652305603\n",
      "708 0.34331774711608887\n",
      "709 0.3433176875114441\n",
      "710 0.34331756830215454\n",
      "711 0.3433174788951874\n",
      "712 0.3433174192905426\n",
      "713 0.34331732988357544\n",
      "714 0.34331727027893066\n",
      "715 0.3433171808719635\n",
      "716 0.3433171212673187\n",
      "717 0.3433170020580292\n",
      "718 0.343316912651062\n",
      "719 0.34331685304641724\n",
      "720 0.3433167636394501\n",
      "721 0.3433166444301605\n",
      "722 0.34331661462783813\n",
      "723 0.34331658482551575\n",
      "724 0.3433164656162262\n",
      "725 0.34331637620925903\n",
      "726 0.34331631660461426\n",
      "727 0.3433162271976471\n",
      "728 0.34331610798835754\n",
      "729 0.343315988779068\n",
      "730 0.3433159589767456\n",
      "731 0.34331589937210083\n",
      "732 0.34331580996513367\n",
      "733 0.3433157205581665\n",
      "734 0.34331566095352173\n",
      "735 0.34331557154655457\n",
      "736 0.3433154821395874\n",
      "737 0.34331539273262024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 0.3433153033256531\n",
      "739 0.3433152437210083\n",
      "740 0.34331515431404114\n",
      "741 0.34331509470939636\n",
      "742 0.3433149755001068\n",
      "743 0.3433149456977844\n",
      "744 0.34331485629081726\n",
      "745 0.3433147668838501\n",
      "746 0.3433147072792053\n",
      "747 0.34331461787223816\n",
      "748 0.3433145582675934\n",
      "749 0.3433144688606262\n",
      "750 0.34331440925598145\n",
      "751 0.3433143198490143\n",
      "752 0.3433142602443695\n",
      "753 0.34331417083740234\n",
      "754 0.3433140814304352\n",
      "755 0.3433140218257904\n",
      "756 0.343313992023468\n",
      "757 0.34331387281417847\n",
      "758 0.3433137834072113\n",
      "759 0.34331372380256653\n",
      "760 0.34331363439559937\n",
      "761 0.3433135747909546\n",
      "762 0.34331345558166504\n",
      "763 0.34331339597702026\n",
      "764 0.3433133363723755\n",
      "765 0.3433133065700531\n",
      "766 0.34331321716308594\n",
      "767 0.343313068151474\n",
      "768 0.3433130383491516\n",
      "769 0.34331294894218445\n",
      "770 0.3433128595352173\n",
      "771 0.3433127999305725\n",
      "772 0.34331271052360535\n",
      "773 0.34331268072128296\n",
      "774 0.3433126211166382\n",
      "775 0.343312531709671\n",
      "776 0.34331244230270386\n",
      "777 0.3433123826980591\n",
      "778 0.3433122932910919\n",
      "779 0.34331226348876953\n",
      "780 0.34331217408180237\n",
      "781 0.3433121144771576\n",
      "782 0.3433120846748352\n",
      "783 0.34331196546554565\n",
      "784 0.3433119058609009\n",
      "785 0.3433118462562561\n",
      "786 0.34331175684928894\n",
      "787 0.34331169724464417\n",
      "788 0.3433115780353546\n",
      "789 0.3433115780353546\n",
      "790 0.34331148862838745\n",
      "791 0.3433113992214203\n",
      "792 0.3433113396167755\n",
      "793 0.3433113098144531\n",
      "794 0.34331122040748596\n",
      "795 0.3433111608028412\n",
      "796 0.343311071395874\n",
      "797 0.3433109521865845\n",
      "798 0.3433109223842621\n",
      "799 0.3433108925819397\n",
      "800 0.34331080317497253\n",
      "801 0.34331074357032776\n",
      "802 0.3433106541633606\n",
      "803 0.3433106243610382\n",
      "804 0.3433105945587158\n",
      "805 0.34331047534942627\n",
      "806 0.3433103859424591\n",
      "807 0.34331032633781433\n",
      "808 0.34331023693084717\n",
      "809 0.3433101773262024\n",
      "810 0.34331008791923523\n",
      "811 0.34330999851226807\n",
      "812 0.3433099389076233\n",
      "813 0.34330981969833374\n",
      "814 0.34330978989601135\n",
      "815 0.3433097004890442\n",
      "816 0.3433096706867218\n",
      "817 0.34330958127975464\n",
      "818 0.34330952167510986\n",
      "819 0.3433094918727875\n",
      "820 0.3433093726634979\n",
      "821 0.3433093726634979\n",
      "822 0.34330928325653076\n",
      "823 0.3433091640472412\n",
      "824 0.3433091342449188\n",
      "825 0.34330904483795166\n",
      "826 0.3433090150356293\n",
      "827 0.3433089554309845\n",
      "828 0.3433088958263397\n",
      "829 0.34330883622169495\n",
      "830 0.3433087468147278\n",
      "831 0.3433087170124054\n",
      "832 0.34330862760543823\n",
      "833 0.34330859780311584\n",
      "834 0.3433084785938263\n",
      "835 0.3433084487915039\n",
      "836 0.3433084189891815\n",
      "837 0.34330829977989197\n",
      "838 0.3433082699775696\n",
      "839 0.3433081805706024\n",
      "840 0.34330815076828003\n",
      "841 0.3433080315589905\n",
      "842 0.3433079421520233\n",
      "843 0.3433079123497009\n",
      "844 0.34330788254737854\n",
      "845 0.3433077931404114\n",
      "846 0.3433077335357666\n",
      "847 0.3433076739311218\n",
      "848 0.34330761432647705\n",
      "849 0.34330758452415466\n",
      "850 0.3433074653148651\n",
      "851 0.34330740571022034\n",
      "852 0.34330737590789795\n",
      "853 0.34330734610557556\n",
      "854 0.3433072566986084\n",
      "855 0.34330713748931885\n",
      "856 0.34330710768699646\n",
      "857 0.3433070778846741\n",
      "858 0.3433069884777069\n",
      "859 0.34330689907073975\n",
      "860 0.34330683946609497\n",
      "861 0.3433068096637726\n",
      "862 0.3433067202568054\n",
      "863 0.34330666065216064\n",
      "864 0.34330663084983826\n",
      "865 0.3433065712451935\n",
      "866 0.3433065116405487\n",
      "867 0.34330642223358154\n",
      "868 0.34330639243125916\n",
      "869 0.3433062732219696\n",
      "870 0.3433062434196472\n",
      "871 0.3433062434196472\n",
      "872 0.34330612421035767\n",
      "873 0.3433060944080353\n",
      "874 0.3433060348033905\n",
      "875 0.34330594539642334\n",
      "876 0.3433058559894562\n",
      "877 0.3433058261871338\n",
      "878 0.343305766582489\n",
      "879 0.34330570697784424\n",
      "880 0.34330567717552185\n",
      "881 0.3433055579662323\n",
      "882 0.3433055579662323\n",
      "883 0.3433054983615875\n",
      "884 0.34330540895462036\n",
      "885 0.3433053493499756\n",
      "886 0.3433053195476532\n",
      "887 0.34330523014068604\n",
      "888 0.34330517053604126\n",
      "889 0.34330514073371887\n",
      "890 0.3433050513267517\n",
      "891 0.3433050215244293\n",
      "892 0.34330493211746216\n",
      "893 0.34330490231513977\n",
      "894 0.3433048725128174\n",
      "895 0.3433047831058502\n",
      "896 0.34330475330352783\n",
      "897 0.34330472350120544\n",
      "898 0.3433046340942383\n",
      "899 0.3433045446872711\n",
      "900 0.34330451488494873\n",
      "901 0.34330448508262634\n",
      "902 0.3433043956756592\n",
      "903 0.3433043360710144\n",
      "904 0.34330427646636963\n",
      "905 0.34330421686172485\n",
      "906 0.3433041274547577\n",
      "907 0.3433040976524353\n",
      "908 0.3433040380477905\n",
      "909 0.34330397844314575\n",
      "910 0.343303918838501\n",
      "911 0.3433038294315338\n",
      "912 0.34330376982688904\n",
      "913 0.34330371022224426\n",
      "914 0.3433036506175995\n",
      "915 0.3433035910129547\n",
      "916 0.3433035612106323\n",
      "917 0.34330353140830994\n",
      "918 0.3433034420013428\n",
      "919 0.343303382396698\n",
      "920 0.3433033227920532\n",
      "921 0.34330329298973083\n",
      "922 0.3433031737804413\n",
      "923 0.3433031439781189\n",
      "924 0.3433031439781189\n",
      "925 0.34330302476882935\n",
      "926 0.34330299496650696\n",
      "927 0.34330296516418457\n",
      "928 0.3433029055595398\n",
      "929 0.343302845954895\n",
      "930 0.34330281615257263\n",
      "931 0.34330272674560547\n",
      "932 0.3433026969432831\n",
      "933 0.3433026075363159\n",
      "934 0.34330257773399353\n",
      "935 0.34330254793167114\n",
      "936 0.34330248832702637\n",
      "937 0.3433024287223816\n",
      "938 0.3433023691177368\n",
      "939 0.34330233931541443\n",
      "940 0.34330227971076965\n",
      "941 0.3433022201061249\n",
      "942 0.3433021903038025\n",
      "943 0.3433021306991577\n",
      "944 0.34330204129219055\n",
      "945 0.34330201148986816\n",
      "946 0.343301922082901\n",
      "947 0.3433018922805786\n",
      "948 0.34330180287361145\n",
      "949 0.34330177307128906\n",
      "950 0.3433017432689667\n",
      "951 0.3433016538619995\n",
      "952 0.3433016240596771\n",
      "953 0.34330159425735474\n",
      "954 0.34330153465270996\n",
      "955 0.3433014750480652\n",
      "956 0.3433014154434204\n",
      "957 0.34330135583877563\n",
      "958 0.34330132603645325\n",
      "959 0.34330126643180847\n",
      "960 0.3433012366294861\n",
      "961 0.3433012068271637\n",
      "962 0.3433011770248413\n",
      "963 0.34330108761787415\n",
      "964 0.34330105781555176\n",
      "965 0.3433009684085846\n",
      "966 0.3433009088039398\n",
      "967 0.34330081939697266\n",
      "968 0.34330078959465027\n",
      "969 0.3433007299900055\n",
      "970 0.3433006703853607\n",
      "971 0.34330064058303833\n",
      "972 0.34330058097839355\n",
      "973 0.34330055117607117\n",
      "974 0.3433005213737488\n",
      "975 0.343300461769104\n",
      "976 0.34330040216445923\n",
      "977 0.34330031275749207\n",
      "978 0.3433002829551697\n",
      "979 0.3433002531528473\n",
      "980 0.3433001637458801\n",
      "981 0.34330013394355774\n",
      "982 0.34330010414123535\n",
      "983 0.3433000445365906\n",
      "984 0.3432999849319458\n",
      "985 0.3432999551296234\n",
      "986 0.34329989552497864\n",
      "987 0.34329986572265625\n",
      "988 0.3432997763156891\n",
      "989 0.3432997465133667\n",
      "990 0.3432997167110443\n",
      "991 0.3432996869087219\n",
      "992 0.34329959750175476\n",
      "993 0.3432995676994324\n",
      "994 0.3432995080947876\n",
      "995 0.3432994484901428\n",
      "996 0.3432994484901428\n",
      "997 0.34329935908317566\n",
      "998 0.34329932928085327\n",
      "999 0.3432992994785309\n"
     ]
    }
   ],
   "source": [
    "for t in range(1000):\n",
    "    y_ = model(torch.from_numpy(features).float())\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(y_, torch.from_numpy(labels).long())\n",
    "    print (t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.from_numpy(features).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.argmax(p.detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
